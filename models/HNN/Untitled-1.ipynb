{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d9b864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f1c1a7d5720>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from itertools import permutations, combinations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SparseLinear(nn.Module):\n",
    "    \"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        connectivity: user defined sparsity matrix\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        coalesce_device: device to coalesce the sparse matrix on\n",
    "            Default: 'gpu'\n",
    "        max_size (int): maximum number of entries allowed before chunking occurrs\n",
    "            Default: 1e8\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> m = nn.SparseLinear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        connectivity,\n",
    "        bias=True,\n",
    "        coalesce_device=\"cuda\",\n",
    "        max_size=1e8,\n",
    "    ):\n",
    "        assert in_features < 2**31 and out_features < 2**31\n",
    "        if connectivity is not None:\n",
    "            assert isinstance(connectivity, torch.LongTensor) or isinstance(\n",
    "                connectivity,\n",
    "                torch.cuda.LongTensor,\n",
    "            ), \"Connectivity must be a Long Tensor\"\n",
    "            assert (\n",
    "                connectivity.shape[0] == 2 and connectivity.shape[1] > 0\n",
    "            ), \"Input shape for connectivity should be (2,nnz)\"\n",
    "            assert (\n",
    "                connectivity.shape[1] <= in_features * out_features\n",
    "            ), \"Nnz can't be bigger than the weight matrix\"\n",
    "        super(SparseLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.connectivity = connectivity\n",
    "        self.max_size = max_size\n",
    "\n",
    "        nnz = connectivity.shape[1]\n",
    "        connectivity = connectivity.to(device=coalesce_device)\n",
    "        indices = connectivity\n",
    "\n",
    "        values = torch.empty(nnz, device=coalesce_device)\n",
    "\n",
    "        self.register_buffer(\"indices\", indices.cpu())\n",
    "        self.weights = nn.Parameter(values.cpu())\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / self.in_features**0.5\n",
    "        nn.init.uniform_(self.weights, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        \"\"\"returns a torch.sparse_coo_tensor view of the underlying weight matrix\n",
    "        This is only for inspection purposes and should not be modified or used in any autograd operations\n",
    "        \"\"\"\n",
    "        weight = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            (self.out_features, self.in_features),\n",
    "        )\n",
    "        return weight.coalesce().detach()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output_shape = list(inputs.shape)\n",
    "        output_shape[-1] = self.out_features\n",
    "\n",
    "        if len(output_shape) == 1:\n",
    "            inputs = inputs.view(1, -1)\n",
    "        inputs = inputs.flatten(end_dim=-2)\n",
    "\n",
    "        target = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            torch.Size([self.out_features, self.in_features]),\n",
    "        )\n",
    "        output = torch.sparse.mm(target, inputs.t()).t()\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output.view(output_shape)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"in_features={}, out_features={}, bias={}, connectivity={}\".format(\n",
    "            self.in_features,\n",
    "            self.out_features,\n",
    "            self.bias is not None,\n",
    "            self.connectivity,\n",
    "        )\n",
    "\n",
    "\n",
    "def separating_cliques(G):\n",
    "    clique_1 = []\n",
    "    clique_2 = []\n",
    "    clique_3 = []\n",
    "    clique_4 = []\n",
    "    for clique in nx.enumerate_all_cliques(G):\n",
    "        clique = set(clique)\n",
    "        if len(clique) == 1:\n",
    "            clique_1.append(clique)\n",
    "        elif len(clique) == 2:\n",
    "            clique_2.append(clique)\n",
    "        elif len(clique) == 3:\n",
    "            clique_3.append(clique)\n",
    "        elif len(clique) == 4:\n",
    "            clique_4.append(clique)\n",
    "    return clique_1, clique_2, clique_3, clique_4\n",
    "\n",
    "\n",
    "def get_connection(clique_last, clique_next):\n",
    "    connection_list = [[], []]\n",
    "    component_mapping = {i: x for i, x in enumerate(clique_last)}\n",
    "    for i, clique in enumerate(clique_next):\n",
    "        component = [set(x) for x in combinations(clique, len(clique) - 1)]\n",
    "        index_next = i\n",
    "        index_last = [\n",
    "            list(component_mapping.keys())[list(component_mapping.values()).index(x)]\n",
    "            for x in component\n",
    "        ]\n",
    "        for j in index_last:\n",
    "            connection_list[0].append(j)\n",
    "            connection_list[1].append(i)\n",
    "\n",
    "    return connection_list\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "# Add 4 nodes\n",
    "G.add_nodes_from([1, 2, 3, 4, 5])\n",
    "# Add 4 edges\n",
    "G.add_edges_from([(1, 2), (2, 3), (2, 4), (3, 4), (4, 5), (3, 5), (2, 5)])\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8826d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 4, 1, 3, 5, 2, 3, 6, 4, 5, 6], [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clique_1, clique_2, clique_3, clique_4 = separating_cliques(G)\n",
    "\n",
    "connection_1 = get_connection(clique_1, clique_2)\n",
    "connection_2 = get_connection(clique_2, clique_3)\n",
    "connection_3 = get_connection(clique_3, clique_4)\n",
    "\n",
    "connection_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e14903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6980, -0.3525, -0.5858, -0.2744, -0.2064,  0.2922, -0.3019]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "len_input = len(np.unique(connection_1[0]))\n",
    "len_output = len(np.unique(connection_1[1]))\n",
    "\n",
    "sl = SparseLinear(\n",
    "    in_features=len_input,\n",
    "    out_features=len_output,\n",
    "    connectivity=torch.tensor([connection_1[1], connection_1[0]], dtype=torch.int64),\n",
    ")\n",
    "x = torch.ones(1, len_input)\n",
    "output = sl(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e3dc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06015173],\n",
       "       [ 0.5625891 ],\n",
       "       [-0.09905863],\n",
       "       [ 0.12613186],\n",
       "       [-0.22061317],\n",
       "       [ 0.6008882 ],\n",
       "       [ 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.weight.to_dense().numpy() @ x.numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8c93f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06015173,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.06583074,  0.12741561,  0.1874524 ,  0.18189035,  0.        ],\n",
       "       [ 0.        , -0.05709397,  0.        ,  0.        , -0.04196466],\n",
       "       [-0.11316157,  0.        ,  0.30750433,  0.        , -0.06821091],\n",
       "       [ 0.        , -0.01736058,  0.        , -0.20325258,  0.        ],\n",
       "       [ 0.2300624 ,  0.3708258 ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.weight.to_dense().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe852b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6d12ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
       "                       [0, 0, 1, 2, 3, 1, 4, 5, 2, 4, 6, 3, 5, 6]]),\n",
       "       values=tensor([-0.0602,  0.0658,  0.1274,  0.1875,  0.1819, -0.0571,\n",
       "                      -0.0420, -0.1132,  0.3075, -0.0682, -0.0174, -0.2033,\n",
       "                       0.2301,  0.3708]),\n",
       "       size=(7, 5), nnz=14, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99c293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of target: torch.Size([7, 5])\n",
      "Shape of inputs.t(): torch.Size([5, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "addmm: index out of column bound: 5 not between 1 and 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(len_input):\n\u001b[1;32m      7\u001b[0m     x[:, i] \u001b[38;5;241m=\u001b[39m x[:, i] \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m output\n",
      "File \u001b[0;32m~/miniconda3/envs/lobframe/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 121\u001b[0m, in \u001b[0;36mSparseLinear.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mflatten(end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    116\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo_tensor(\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices,\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights,\n\u001b[1;32m    119\u001b[0m     torch\u001b[38;5;241m.\u001b[39mSize([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features]),\n\u001b[1;32m    120\u001b[0m )\n\u001b[0;32m--> 121\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: addmm: index out of column bound: 5 not between 1 and 5"
     ]
    }
   ],
   "source": [
    "num_batches = 3\n",
    "x = torch.ones(3, len_input)\n",
    "x[1, :] = 2\n",
    "x[2, :] = 3\n",
    "\n",
    "for i in range(len_input):\n",
    "    x[:, i] = x[:, i] + i / 10\n",
    "\n",
    "output = sl(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da128073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 100])\n",
      "Output shape: torch.Size([4, 8, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvFilter(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            padding=0,  # No padding for exact 2x downsampling\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, height, width)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "batch_size = 4\n",
    "in_channels = 1\n",
    "out_channels = 8\n",
    "rows = 100\n",
    "cols = 6\n",
    "\n",
    "# Create model\n",
    "conv1_tetrahedra = nn.Sequential(\n",
    "    nn.Conv1d(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=8,\n",
    "        kernel_size=2,\n",
    "        stride=2,\n",
    "    ),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "# Create batch of images\n",
    "x = torch.randn(batch_size, rows)\n",
    "print(f\"Input shape: {x.shape}\")  # torch.Size([4, 3, 32, 32])\n",
    "\n",
    "x = x.unsqueeze(1)\n",
    "\n",
    "# Apply convolution\n",
    "output = conv1_tetrahedra(x)\n",
    "print(f\"Output shape: {output.shape}\")  # torch.Size([4, 16, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "777a7e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2747de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 400])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3955f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f5c48bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7730, 0.0756, 0.0871, 0.0632, 0.0000, 0.1362, 0.0000, 0.2796, 0.1956,\n",
      "         0.0000, 0.2654, 0.0000]], grad_fn=<CatBackward0>)\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class HNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        num_edges: int,\n",
    "        num_triangles: int,\n",
    "        num_tetrahedra: int,\n",
    "        nodes_to_edges_connections: tuple,\n",
    "        edges_to_triangles_connections: tuple,\n",
    "        triangles_to_tetrahedra_connections: tuple,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        nodes_to_edges_connections: tuple of two lists, where the first list contains the indices of the edges\n",
    "        and the second list contains the indices of the nodes connected to those edges, such that the i-th node\n",
    "        in the first list is a member of the i-th edge in the second list.\n",
    "\n",
    "        Same for edges_to_triangles_connections and triangles_to_tetrahedra_connections\n",
    "        \"\"\"\n",
    "        super(HNN, self).__init__()\n",
    "        self.sparse_layer_edges = SparseLinear(\n",
    "            num_nodes,\n",
    "            num_edges,\n",
    "            connectivity=torch.tensor(\n",
    "                [nodes_to_edges_connections[1], nodes_to_edges_connections[0]],\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.sparse_layer_triangles = SparseLinear(\n",
    "            num_edges,\n",
    "            num_triangles,\n",
    "            connectivity=torch.tensor(\n",
    "                [edges_to_triangles_connections[1], edges_to_triangles_connections[0]],\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.triangles_to_tetrahedra_connections = triangles_to_tetrahedra_connections\n",
    "\n",
    "        if len(self.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            self.sparse_layer_tetrahedra = SparseLinear(\n",
    "                num_triangles,\n",
    "                num_tetrahedra,\n",
    "                connectivity=torch.tensor(\n",
    "                    [\n",
    "                        triangles_to_tetrahedra_connections[1],\n",
    "                        triangles_to_tetrahedra_connections[0],\n",
    "                    ],\n",
    "                    dtype=torch.int64,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.sparse_layer_tetrahedra = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s1 = F.relu(self.sparse_layer_edges(x))\n",
    "\n",
    "        x_s2 = F.relu(self.sparse_layer_triangles(x_s1))\n",
    "\n",
    "        if len(self.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            x_s3 = F.relu(self.sparse_layer_tetrahedra(x_s2))\n",
    "\n",
    "            return torch.cat([x_s1, x_s2, x_s3], 1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return torch.cat([x_s1, x_s2], 1)\n",
    "\n",
    "\n",
    "hnn = HNN(\n",
    "    num_nodes=len(clique_1),\n",
    "    num_edges=len(clique_2),\n",
    "    num_triangles=len(clique_3),\n",
    "    num_tetrahedra=len(clique_4),\n",
    "    nodes_to_edges_connections=connection_1,\n",
    "    edges_to_triangles_connections=connection_2,\n",
    "    triangles_to_tetrahedra_connections=connection_3,\n",
    ")\n",
    "\n",
    "x = torch.ones(1, len(clique_1))\n",
    "output = hnn(x)\n",
    "print(output)\n",
    "print(output.shape)  # Should print the shape of the output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ca78976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clique_2), len(clique_3), len(clique_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e484a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(connection_1[1]) + 1, max(connection_2[1]) + 1, max(connection_3[1]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1854af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SparseLinear(nn.Module):\n",
    "    \"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        connectivity: user defined sparsity matrix\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        coalesce_device: device to coalesce the sparse matrix on\n",
    "            Default: 'gpu'\n",
    "        max_size (int): maximum number of entries allowed before chunking occurrs\n",
    "            Default: 1e8\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> m = nn.SparseLinear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        connectivity,\n",
    "        bias=True,\n",
    "        coalesce_device=\"cuda\",\n",
    "        max_size=1e8,\n",
    "    ):\n",
    "        assert in_features < 2**31 and out_features < 2**31\n",
    "        if connectivity is not None:\n",
    "            assert isinstance(connectivity, torch.LongTensor) or isinstance(\n",
    "                connectivity,\n",
    "                torch.cuda.LongTensor,\n",
    "            ), \"Connectivity must be a Long Tensor\"\n",
    "            assert (\n",
    "                connectivity.shape[0] == 2 and connectivity.shape[1] > 0\n",
    "            ), \"Input shape for connectivity should be (2,nnz)\"\n",
    "            assert (\n",
    "                connectivity.shape[1] <= in_features * out_features\n",
    "            ), \"Nnz can't be bigger than the weight matrix\"\n",
    "        super(SparseLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.connectivity = connectivity\n",
    "        self.max_size = max_size\n",
    "\n",
    "        nnz = connectivity.shape[1]\n",
    "        connectivity = connectivity.to(device=coalesce_device)\n",
    "        indices = connectivity\n",
    "\n",
    "        values = torch.empty(nnz, device=coalesce_device)\n",
    "\n",
    "        self.register_buffer(\"indices\", indices.cpu())\n",
    "        self.weights = nn.Parameter(values.cpu())\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / self.in_features**0.5\n",
    "        nn.init.uniform_(self.weights, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        \"\"\"returns a torch.sparse_coo_tensor view of the underlying weight matrix\n",
    "        This is only for inspection purposes and should not be modified or used in any autograd operations\n",
    "        \"\"\"\n",
    "        weight = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            (self.out_features, self.in_features),\n",
    "        )\n",
    "        return weight.coalesce().detach()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output_shape = list(inputs.shape)\n",
    "        output_shape[-1] = self.out_features\n",
    "\n",
    "        if len(output_shape) == 1:\n",
    "            inputs = inputs.view(1, -1)\n",
    "        inputs = inputs.flatten(end_dim=-2)\n",
    "\n",
    "        target = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            torch.Size([self.out_features, self.in_features]),\n",
    "        )\n",
    "        output = torch.sparse.mm(target, inputs.t()).t()\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output.view(output_shape)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"in_features={}, out_features={}, bias={}, connectivity={}\".format(\n",
    "            self.in_features,\n",
    "            self.out_features,\n",
    "            self.bias is not None,\n",
    "            self.connectivity,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GraphHomologicalStructure:\n",
    "    nodes_to_edges_connections: tuple\n",
    "    edges_to_triangles_connections: tuple\n",
    "    triangles_to_tetrahedra_connections: tuple\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self) -> int:\n",
    "        return max(self.nodes_to_edges_connections[0]) + 1\n",
    "\n",
    "    @property\n",
    "    def num_edges(self) -> int:\n",
    "        return max(self.edges_to_triangles_connections[0]) + 1\n",
    "\n",
    "    @property\n",
    "    def num_triangles(self) -> int:\n",
    "        return (\n",
    "            max(self.triangles_to_tetrahedra_connections[0]) + 1\n",
    "            if self.triangles_to_tetrahedra_connections\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_tetrahedra(self) -> int:\n",
    "        return (\n",
    "            max(self.triangles_to_tetrahedra_connections[1]) + 1\n",
    "            if self.triangles_to_tetrahedra_connections\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    def get_nodes_to_edges_connections_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                self.nodes_to_edges_connections[1],\n",
    "                self.nodes_to_edges_connections[0],\n",
    "            ],\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "\n",
    "    def get_edges_to_triangles_connections_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                self.edges_to_triangles_connections[1],\n",
    "                self.edges_to_triangles_connections[0],\n",
    "            ],\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "\n",
    "    def get_triangles_to_tetrahedra_connections_tensor(self) -> torch.Tensor:\n",
    "        return (\n",
    "            torch.tensor(\n",
    "                [\n",
    "                    self.triangles_to_tetrahedra_connections[1],\n",
    "                    self.triangles_to_tetrahedra_connections[0],\n",
    "                ],\n",
    "                dtype=torch.int64,\n",
    "            )\n",
    "            if self.triangles_to_tetrahedra_connections\n",
    "            else torch.empty((2, 0), dtype=torch.int64)\n",
    "        )\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return GraphHomologicalStructure(\n",
    "            nodes_to_edges_connections=deepcopy(self.nodes_to_edges_connections, memo),\n",
    "            edges_to_triangles_connections=deepcopy(\n",
    "                self.edges_to_triangles_connections, memo\n",
    "            ),\n",
    "            triangles_to_tetrahedra_connections=deepcopy(\n",
    "                self.triangles_to_tetrahedra_connections, memo\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class HNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        homological_structure: GraphHomologicalStructure,\n",
    "    ):\n",
    "        super(HNN, self).__init__()\n",
    "        self.homological_structure = homological_structure\n",
    "\n",
    "        self.sparse_layer_edges = SparseLinear(\n",
    "            homological_structure.num_nodes,\n",
    "            homological_structure.num_edges,\n",
    "            connectivity=self.homological_structure.get_nodes_to_edges_connections_tensor(),\n",
    "        )\n",
    "\n",
    "        self.sparse_layer_triangles = SparseLinear(\n",
    "            self.homological_structure.num_edges,\n",
    "            self.homological_structure.num_triangles,\n",
    "            connectivity=self.homological_structure.get_edges_to_triangles_connections_tensor(),\n",
    "        )\n",
    "\n",
    "        if len(self.homological_structure.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            self.sparse_layer_tetrahedra = SparseLinear(\n",
    "                self.homological_structure.num_triangles,\n",
    "                self.homological_structure.num_tetrahedra,\n",
    "                connectivity=self.homological_structure.get_triangles_to_tetrahedra_connections_tensor(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.sparse_layer_tetrahedra = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s1 = F.relu(self.sparse_layer_edges(x))\n",
    "\n",
    "        x_s2 = F.relu(self.sparse_layer_triangles(x_s1))\n",
    "\n",
    "        if len(self.homological_structure.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            x_s3 = F.relu(self.sparse_layer_tetrahedra(x_s2))\n",
    "\n",
    "            return torch.cat([x_s1, x_s2, x_s3], 1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return torch.cat([x_s1, x_s2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0cf9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutedMixingHNN(nn.Module):\n",
    "    @staticmethod\n",
    "    def get_connections_for_convoluted_mixing_hnn(\n",
    "        nodes_to_edges_connections: tuple,\n",
    "        num_convolutional_channels: int,\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        This function modifies the connections for the convoluted mixing HNN.\n",
    "        It expands the nodes_to_edges_connections to account for the convolutional channels.\n",
    "        \"\"\"\n",
    "        new_nodes_to_edges_connections = ([], [])\n",
    "        for connection_index in range(len(nodes_to_edges_connections[0])):\n",
    "            node_index = nodes_to_edges_connections[0][connection_index]\n",
    "            edge_index = nodes_to_edges_connections[1][connection_index]\n",
    "\n",
    "            for channel in range(num_convolutional_channels):\n",
    "                new_nodes_to_edges_connections[0].append(\n",
    "                    node_index * num_convolutional_channels + channel\n",
    "                )\n",
    "                new_nodes_to_edges_connections[1].append(edge_index)\n",
    "\n",
    "        return new_nodes_to_edges_connections\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        homological_structure: GraphHomologicalStructure,\n",
    "        num_convolutional_channels: int,\n",
    "        lighten: bool = False,\n",
    "    ):\n",
    "        super(ConvolutedMixingHNN, self).__init__()\n",
    "        self.name = \"hcnn\"\n",
    "        if lighten:\n",
    "            self.name += \"-lighten\"\n",
    "\n",
    "        self.homological_structure = homological_structure\n",
    "\n",
    "        self.conv_layer_price_vol = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=1,\n",
    "                out_channels=num_convolutional_channels,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        convoluted_nodes_to_edges_connections = (\n",
    "            self.get_connections_for_convoluted_mixing_hnn(\n",
    "                homological_structure.nodes_to_edges_connections,\n",
    "                num_convolutional_channels,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.convoluted_homological_structure = deepcopy(homological_structure)\n",
    "        self.convoluted_homological_structure.nodes_to_edges_connections = (\n",
    "            convoluted_nodes_to_edges_connections\n",
    "        )\n",
    "\n",
    "        self.hnn = HNN(self.convoluted_homological_structure)\n",
    "\n",
    "        self.readout_layer = nn.Linear(\n",
    "            in_features=homological_structure.num_edges\n",
    "            + homological_structure.num_triangles\n",
    "            + homological_structure.num_tetrahedra,\n",
    "            out_features=3,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  x.shape = (batch_size, 1, num_features) num_features è della dimensione di tutti i nodi (nodi nel senso di spazio-temporali, quindi vol1ask_lag0, vol1ask_lag1, ...) * 2 perche c'è price and volume\n",
    "\n",
    "        # after conv_layer_price_vol -> x.shape = (batch_size, num_convolutional_channels, num_features // 2)\n",
    "        x = self.conv_layer_price_vol(x)\n",
    "\n",
    "        # after flatten -> # x.shape = (batch_size, num_convolutional_channels * num_features // 2)\n",
    "        # Permute to have channels first, then flatten. so the columns will be feature_channel1, feature_channel2, ..., feature_channelN\n",
    "        x = x.permute(0, 2, 1).flatten(start_dim=1)\n",
    "\n",
    "        x = self.hnn(x)  # x.shape = (batch_size, num_classes)\n",
    "\n",
    "        # after hnn -> x.shape = (batch_size, num_edges + num_triangles + num_tetrahedra)\n",
    "        x = self.readout_layer(x)  # x.shape = (batch_size, num\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0b35faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmhnn = ConvolutedMixingHNN(\n",
    "    homological_structure=GraphHomologicalStructure(\n",
    "        nodes_to_edges_connections=connection_1,\n",
    "        edges_to_triangles_connections=connection_2,\n",
    "        triangles_to_tetrahedra_connections=connection_3,\n",
    "    ),\n",
    "    num_convolutional_channels=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a39fa1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2990,  1.8206,  0.8336, -2.0264, -0.4700, -1.0977,  0.6162,\n",
       "           0.7628,  0.2390,  1.5506]],\n",
       "\n",
       "        [[ 1.5205, -0.3781,  1.6900,  1.0291,  0.2050,  0.5757,  0.8598,\n",
       "           0.5767,  1.0066,  0.7828]],\n",
       "\n",
       "        [[ 0.1221,  0.3215,  1.2858,  0.1554, -0.4019,  1.5746,  0.0791,\n",
       "          -0.0583,  1.1217, -0.2726]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FEATURES = G.number_of_nodes() * 2\n",
    "x = torch.randn(3, 1, N_FEATURES)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6425b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1222,  0.0877, -0.1908],\n",
      "        [-0.0963,  0.1122, -0.1353],\n",
      "        [-0.0119,  0.1761, -0.0889]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = cmhnn(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94d8bf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lobframe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
