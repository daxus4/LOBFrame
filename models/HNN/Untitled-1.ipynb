{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d9b864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f536c7f3640>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from itertools import permutations, combinations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SparseLinear(nn.Module):\n",
    "    \"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        connectivity: user defined sparsity matrix\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        coalesce_device: device to coalesce the sparse matrix on\n",
    "            Default: 'gpu'\n",
    "        max_size (int): maximum number of entries allowed before chunking occurrs\n",
    "            Default: 1e8\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> m = nn.SparseLinear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        connectivity,\n",
    "        bias=True,\n",
    "        coalesce_device=\"cuda\",\n",
    "        max_size=1e8,\n",
    "    ):\n",
    "        assert in_features < 2**31 and out_features < 2**31\n",
    "        if connectivity is not None:\n",
    "            assert isinstance(connectivity, torch.LongTensor) or isinstance(\n",
    "                connectivity,\n",
    "                torch.cuda.LongTensor,\n",
    "            ), \"Connectivity must be a Long Tensor\"\n",
    "            assert (\n",
    "                connectivity.shape[0] == 2 and connectivity.shape[1] > 0\n",
    "            ), \"Input shape for connectivity should be (2,nnz)\"\n",
    "            assert (\n",
    "                connectivity.shape[1] <= in_features * out_features\n",
    "            ), \"Nnz can't be bigger than the weight matrix\"\n",
    "        super(SparseLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.connectivity = connectivity\n",
    "        self.max_size = max_size\n",
    "\n",
    "        nnz = connectivity.shape[1]\n",
    "        connectivity = connectivity.to(device=coalesce_device)\n",
    "        indices = connectivity\n",
    "\n",
    "        values = torch.empty(nnz, device=coalesce_device)\n",
    "\n",
    "        self.register_buffer(\"indices\", indices.cpu())\n",
    "        self.weights = nn.Parameter(values.cpu())\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / self.in_features**0.5\n",
    "        nn.init.uniform_(self.weights, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        \"\"\"returns a torch.sparse_coo_tensor view of the underlying weight matrix\n",
    "        This is only for inspection purposes and should not be modified or used in any autograd operations\n",
    "        \"\"\"\n",
    "        weight = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            (self.out_features, self.in_features),\n",
    "        )\n",
    "        return weight.coalesce().detach()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output_shape = list(inputs.shape)\n",
    "        output_shape[-1] = self.out_features\n",
    "\n",
    "        if len(output_shape) == 1:\n",
    "            inputs = inputs.view(1, -1)\n",
    "        inputs = inputs.flatten(end_dim=-2)\n",
    "\n",
    "        target = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            torch.Size([self.out_features, self.in_features]),\n",
    "        )\n",
    "        output = torch.sparse.mm(target, inputs.t()).t()\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output.view(output_shape)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"in_features={}, out_features={}, bias={}, connectivity={}\".format(\n",
    "            self.in_features,\n",
    "            self.out_features,\n",
    "            self.bias is not None,\n",
    "            self.connectivity,\n",
    "        )\n",
    "\n",
    "\n",
    "def separating_cliques(G):\n",
    "    clique_1 = []\n",
    "    clique_2 = []\n",
    "    clique_3 = []\n",
    "    clique_4 = []\n",
    "    for clique in nx.enumerate_all_cliques(G):\n",
    "        clique = set(clique)\n",
    "        if len(clique) == 1:\n",
    "            clique_1.append(clique)\n",
    "        elif len(clique) == 2:\n",
    "            clique_2.append(clique)\n",
    "        elif len(clique) == 3:\n",
    "            clique_3.append(clique)\n",
    "        elif len(clique) == 4:\n",
    "            clique_4.append(clique)\n",
    "    return clique_1, clique_2, clique_3, clique_4\n",
    "\n",
    "\n",
    "def get_connection(clique_last, clique_next):\n",
    "    connection_list = [[], []]\n",
    "    component_mapping = {i: x for i, x in enumerate(clique_last)}\n",
    "    for i, clique in enumerate(clique_next):\n",
    "        component = [set(x) for x in combinations(clique, len(clique) - 1)]\n",
    "        index_next = i\n",
    "        index_last = [\n",
    "            list(component_mapping.keys())[list(component_mapping.values()).index(x)]\n",
    "            for x in component\n",
    "        ]\n",
    "        for j in index_last:\n",
    "            connection_list[0].append(j)\n",
    "            connection_list[1].append(i)\n",
    "\n",
    "    return connection_list\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "# Add 4 nodes\n",
    "G.add_nodes_from([1, 2, 3, 4, 5])\n",
    "# Add 4 edges\n",
    "G.add_edges_from([(1, 2), (2, 3), (2, 4), (3, 4), (4, 5), (3, 5), (2, 5)])\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8826d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 4, 1, 3, 5, 2, 3, 6, 4, 5, 6], [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clique_1, clique_2, clique_3, clique_4 = separating_cliques(G)\n",
    "\n",
    "connection_1 = get_connection(clique_1, clique_2)\n",
    "connection_2 = get_connection(clique_2, clique_3)\n",
    "connection_3 = get_connection(clique_3, clique_4)\n",
    "\n",
    "connection_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c27f0794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0},\n",
       " {1},\n",
       " {2},\n",
       " {3},\n",
       " {4},\n",
       " {5},\n",
       " {6},\n",
       " {7},\n",
       " {8},\n",
       " {9},\n",
       " {10},\n",
       " {11},\n",
       " {12},\n",
       " {13},\n",
       " {14},\n",
       " {15},\n",
       " {16},\n",
       " {17},\n",
       " {18},\n",
       " {19},\n",
       " {20},\n",
       " {21},\n",
       " {22},\n",
       " {23},\n",
       " {24},\n",
       " {25},\n",
       " {26},\n",
       " {27},\n",
       " {28},\n",
       " {29},\n",
       " {30},\n",
       " {31},\n",
       " {32},\n",
       " {33},\n",
       " {34},\n",
       " {35},\n",
       " {36},\n",
       " {37},\n",
       " {38},\n",
       " {39},\n",
       " {40},\n",
       " {41},\n",
       " {42},\n",
       " {43},\n",
       " {44},\n",
       " {45},\n",
       " {46},\n",
       " {47},\n",
       " {48},\n",
       " {49},\n",
       " {50},\n",
       " {51},\n",
       " {52},\n",
       " {53},\n",
       " {54},\n",
       " {55},\n",
       " {56},\n",
       " {57},\n",
       " {58},\n",
       " {59},\n",
       " {60},\n",
       " {61},\n",
       " {62},\n",
       " {63},\n",
       " {64},\n",
       " {65},\n",
       " {66},\n",
       " {67},\n",
       " {68},\n",
       " {69},\n",
       " {70},\n",
       " {71},\n",
       " {72},\n",
       " {73},\n",
       " {74},\n",
       " {75},\n",
       " {76},\n",
       " {77},\n",
       " {78},\n",
       " {79},\n",
       " {80},\n",
       " {81},\n",
       " {82},\n",
       " {83},\n",
       " {84},\n",
       " {85},\n",
       " {86},\n",
       " {87},\n",
       " {88},\n",
       " {89},\n",
       " {90},\n",
       " {91},\n",
       " {92},\n",
       " {93},\n",
       " {94},\n",
       " {95},\n",
       " {96},\n",
       " {97},\n",
       " {98},\n",
       " {99}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clique_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e14903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4475,  0.5695, -0.4326,  0.2320, -0.7304,  0.2701,  0.0411]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "len_input = len(np.unique(connection_1[0]))\n",
    "len_output = len(np.unique(connection_1[1]))\n",
    "\n",
    "sl = SparseLinear(\n",
    "    in_features=len_input,\n",
    "    out_features=len_output,\n",
    "    connectivity=torch.tensor([connection_1[1], connection_1[0]], dtype=torch.int64),\n",
    ")\n",
    "x = torch.ones(1, len_input)\n",
    "output = sl(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e3dc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06015173],\n",
       "       [ 0.5625891 ],\n",
       "       [-0.09905863],\n",
       "       [ 0.12613186],\n",
       "       [-0.22061317],\n",
       "       [ 0.6008882 ],\n",
       "       [ 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.weight.to_dense().numpy() @ x.numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8c93f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06015173,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.06583074,  0.12741561,  0.1874524 ,  0.18189035,  0.        ],\n",
       "       [ 0.        , -0.05709397,  0.        ,  0.        , -0.04196466],\n",
       "       [-0.11316157,  0.        ,  0.30750433,  0.        , -0.06821091],\n",
       "       [ 0.        , -0.01736058,  0.        , -0.20325258,  0.        ],\n",
       "       [ 0.2300624 ,  0.3708258 ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.weight.to_dense().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe852b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6d12ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
       "                       [0, 0, 1, 2, 3, 1, 4, 5, 2, 4, 6, 3, 5, 6]]),\n",
       "       values=tensor([-0.0602,  0.0658,  0.1274,  0.1875,  0.1819, -0.0571,\n",
       "                      -0.0420, -0.1132,  0.3075, -0.0682, -0.0174, -0.2033,\n",
       "                       0.2301,  0.3708]),\n",
       "       size=(7, 5), nnz=14, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99c293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of target: torch.Size([7, 5])\n",
      "Shape of inputs.t(): torch.Size([5, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "addmm: index out of column bound: 5 not between 1 and 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(len_input):\n\u001b[1;32m      7\u001b[0m     x[:, i] \u001b[38;5;241m=\u001b[39m x[:, i] \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m output\n",
      "File \u001b[0;32m~/miniconda3/envs/lobframe/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 121\u001b[0m, in \u001b[0;36mSparseLinear.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mflatten(end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    116\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo_tensor(\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices,\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights,\n\u001b[1;32m    119\u001b[0m     torch\u001b[38;5;241m.\u001b[39mSize([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features]),\n\u001b[1;32m    120\u001b[0m )\n\u001b[0;32m--> 121\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: addmm: index out of column bound: 5 not between 1 and 5"
     ]
    }
   ],
   "source": [
    "num_batches = 3\n",
    "x = torch.ones(3, len_input)\n",
    "x[1, :] = 2\n",
    "x[2, :] = 3\n",
    "\n",
    "for i in range(len_input):\n",
    "    x[:, i] = x[:, i] + i / 10\n",
    "\n",
    "output = sl(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da128073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 100])\n",
      "Output shape: torch.Size([4, 8, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvFilter(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            padding=0,  # No padding for exact 2x downsampling\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, height, width)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "batch_size = 4\n",
    "in_channels = 1\n",
    "out_channels = 8\n",
    "rows = 100\n",
    "cols = 6\n",
    "\n",
    "# Create model\n",
    "conv1_tetrahedra = nn.Sequential(\n",
    "    nn.Conv1d(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=8,\n",
    "        kernel_size=2,\n",
    "        stride=2,\n",
    "    ),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "# Create batch of images\n",
    "x = torch.randn(batch_size, rows)\n",
    "print(f\"Input shape: {x.shape}\")  # torch.Size([4, 3, 32, 32])\n",
    "\n",
    "x = x.unsqueeze(1)\n",
    "\n",
    "# Apply convolution\n",
    "output = conv1_tetrahedra(x)\n",
    "print(f\"Output shape: {output.shape}\")  # torch.Size([4, 16, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "777a7e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d2747de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 400])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5c48bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.3031, 0.0000, 0.3026, 0.1994, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000]], grad_fn=<CatBackward0>)\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class HNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        num_edges: int,\n",
    "        num_triangles: int,\n",
    "        num_tetrahedra: int,\n",
    "        nodes_to_edges_connections: tuple,\n",
    "        edges_to_triangles_connections: tuple,\n",
    "        triangles_to_tetrahedra_connections: tuple,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        nodes_to_edges_connections: tuple of two lists, where the first list contains the indices of the edges\n",
    "        and the second list contains the indices of the nodes connected to those edges, such that the i-th node\n",
    "        in the first list is a member of the i-th edge in the second list.\n",
    "\n",
    "        Same for edges_to_triangles_connections and triangles_to_tetrahedra_connections\n",
    "        \"\"\"\n",
    "        super(HNN, self).__init__()\n",
    "        self.sparse_layer_edges = SparseLinear(\n",
    "            num_nodes,\n",
    "            num_edges,\n",
    "            connectivity=torch.tensor(\n",
    "                [nodes_to_edges_connections[1], nodes_to_edges_connections[0]],\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.sparse_layer_triangles = SparseLinear(\n",
    "            num_edges,\n",
    "            num_triangles,\n",
    "            connectivity=torch.tensor(\n",
    "                [edges_to_triangles_connections[1], edges_to_triangles_connections[0]],\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.triangles_to_tetrahedra_connections = triangles_to_tetrahedra_connections\n",
    "\n",
    "        if len(self.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            self.sparse_layer_tetrahedra = SparseLinear(\n",
    "                num_triangles,\n",
    "                num_tetrahedra,\n",
    "                connectivity=torch.tensor(\n",
    "                    [\n",
    "                        triangles_to_tetrahedra_connections[1],\n",
    "                        triangles_to_tetrahedra_connections[0],\n",
    "                    ],\n",
    "                    dtype=torch.int64,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.sparse_layer_tetrahedra = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s1 = F.relu(self.sparse_layer_edges(x))\n",
    "\n",
    "        x_s2 = F.relu(self.sparse_layer_triangles(x_s1))\n",
    "\n",
    "        if len(self.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            x_s3 = F.relu(self.sparse_layer_tetrahedra(x_s2))\n",
    "\n",
    "            return torch.cat([x_s1, x_s2, x_s3], 1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return torch.cat([x_s1, x_s2], 1)\n",
    "\n",
    "\n",
    "hnn = HNN(\n",
    "    num_nodes=len(clique_1),\n",
    "    num_edges=len(clique_2),\n",
    "    num_triangles=len(clique_3),\n",
    "    num_tetrahedra=len(clique_4),\n",
    "    nodes_to_edges_connections=connection_1,\n",
    "    edges_to_triangles_connections=connection_2,\n",
    "    triangles_to_tetrahedra_connections=connection_3,\n",
    ")\n",
    "\n",
    "x = torch.ones(1, len(clique_1))\n",
    "output = hnn(x)\n",
    "print(output)\n",
    "print(output.shape)  # Should print the shape of the output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ca78976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clique_2), len(clique_3), len(clique_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e484a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(connection_1[1]) + 1, max(connection_2[1]) + 1, max(connection_3[1]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1854af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SparseLinear(nn.Module):\n",
    "    \"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        connectivity: user defined sparsity matrix\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        coalesce_device: device to coalesce the sparse matrix on\n",
    "            Default: 'gpu'\n",
    "        max_size (int): maximum number of entries allowed before chunking occurrs\n",
    "            Default: 1e8\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> m = nn.SparseLinear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        connectivity,\n",
    "        bias=True,\n",
    "        coalesce_device=\"cuda\",\n",
    "        max_size=1e8,\n",
    "    ):\n",
    "        assert in_features < 2**31 and out_features < 2**31\n",
    "        if connectivity is not None:\n",
    "            assert isinstance(connectivity, torch.LongTensor) or isinstance(\n",
    "                connectivity,\n",
    "                torch.cuda.LongTensor,\n",
    "            ), \"Connectivity must be a Long Tensor\"\n",
    "            assert (\n",
    "                connectivity.shape[0] == 2 and connectivity.shape[1] > 0\n",
    "            ), \"Input shape for connectivity should be (2,nnz)\"\n",
    "            assert (\n",
    "                connectivity.shape[1] <= in_features * out_features\n",
    "            ), \"Nnz can't be bigger than the weight matrix\"\n",
    "        super(SparseLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.connectivity = connectivity\n",
    "        self.max_size = max_size\n",
    "\n",
    "        nnz = connectivity.shape[1]\n",
    "        connectivity = connectivity.to(device=coalesce_device)\n",
    "        indices = connectivity\n",
    "\n",
    "        values = torch.empty(nnz, device=coalesce_device)\n",
    "\n",
    "        self.register_buffer(\"indices\", indices.cpu())\n",
    "        self.weights = nn.Parameter(values.cpu())\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / self.in_features**0.5\n",
    "        nn.init.uniform_(self.weights, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        \"\"\"returns a torch.sparse_coo_tensor view of the underlying weight matrix\n",
    "        This is only for inspection purposes and should not be modified or used in any autograd operations\n",
    "        \"\"\"\n",
    "        weight = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            (self.out_features, self.in_features),\n",
    "        )\n",
    "        return weight.coalesce().detach()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output_shape = list(inputs.shape)\n",
    "        output_shape[-1] = self.out_features\n",
    "\n",
    "        if len(output_shape) == 1:\n",
    "            inputs = inputs.view(1, -1)\n",
    "        inputs = inputs.flatten(end_dim=-2)\n",
    "\n",
    "        target = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            torch.Size([self.out_features, self.in_features]),\n",
    "        )\n",
    "        output = torch.sparse.mm(target, inputs.t()).t()\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output.view(output_shape)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"in_features={}, out_features={}, bias={}, connectivity={}\".format(\n",
    "            self.in_features,\n",
    "            self.out_features,\n",
    "            self.bias is not None,\n",
    "            self.connectivity,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GraphHomologicalStructure:\n",
    "    nodes_to_edges_connections: tuple\n",
    "    edges_to_triangles_connections: tuple\n",
    "    triangles_to_tetrahedra_connections: tuple\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self) -> int:\n",
    "        return max(self.nodes_to_edges_connections[0]) + 1\n",
    "\n",
    "    @property\n",
    "    def num_edges(self) -> int:\n",
    "        return max(self.edges_to_triangles_connections[0]) + 1\n",
    "\n",
    "    @property\n",
    "    def num_triangles(self) -> int:\n",
    "        return (\n",
    "            max(self.triangles_to_tetrahedra_connections[0]) + 1\n",
    "            if self.triangles_to_tetrahedra_connections\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_tetrahedra(self) -> int:\n",
    "        return (\n",
    "            max(self.triangles_to_tetrahedra_connections[1]) + 1\n",
    "            if self.triangles_to_tetrahedra_connections\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    def get_nodes_to_edges_connections_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                self.nodes_to_edges_connections[1],\n",
    "                self.nodes_to_edges_connections[0],\n",
    "            ],\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "\n",
    "    def get_edges_to_triangles_connections_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                self.edges_to_triangles_connections[1],\n",
    "                self.edges_to_triangles_connections[0],\n",
    "            ],\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "\n",
    "    def get_triangles_to_tetrahedra_connections_tensor(self) -> torch.Tensor:\n",
    "        return (\n",
    "            torch.tensor(\n",
    "                [\n",
    "                    self.triangles_to_tetrahedra_connections[1],\n",
    "                    self.triangles_to_tetrahedra_connections[0],\n",
    "                ],\n",
    "                dtype=torch.int64,\n",
    "            )\n",
    "            if self.triangles_to_tetrahedra_connections\n",
    "            else torch.empty((2, 0), dtype=torch.int64)\n",
    "        )\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return GraphHomologicalStructure(\n",
    "            nodes_to_edges_connections=deepcopy(self.nodes_to_edges_connections, memo),\n",
    "            edges_to_triangles_connections=deepcopy(\n",
    "                self.edges_to_triangles_connections, memo\n",
    "            ),\n",
    "            triangles_to_tetrahedra_connections=deepcopy(\n",
    "                self.triangles_to_tetrahedra_connections, memo\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class HNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        homological_structure: GraphHomologicalStructure,\n",
    "    ):\n",
    "        super(HNN, self).__init__()\n",
    "        self.homological_structure = homological_structure\n",
    "\n",
    "        self.sparse_layer_edges = SparseLinear(\n",
    "            homological_structure.num_nodes,\n",
    "            homological_structure.num_edges,\n",
    "            connectivity=self.homological_structure.get_nodes_to_edges_connections_tensor(),\n",
    "        )\n",
    "\n",
    "        self.sparse_layer_triangles = SparseLinear(\n",
    "            self.homological_structure.num_edges,\n",
    "            self.homological_structure.num_triangles,\n",
    "            connectivity=self.homological_structure.get_edges_to_triangles_connections_tensor(),\n",
    "        )\n",
    "\n",
    "        if len(self.homological_structure.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            self.sparse_layer_tetrahedra = SparseLinear(\n",
    "                self.homological_structure.num_triangles,\n",
    "                self.homological_structure.num_tetrahedra,\n",
    "                connectivity=self.homological_structure.get_triangles_to_tetrahedra_connections_tensor(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.sparse_layer_tetrahedra = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s1 = F.relu(self.sparse_layer_edges(x))\n",
    "\n",
    "        x_s2 = F.relu(self.sparse_layer_triangles(x_s1))\n",
    "\n",
    "        if len(self.homological_structure.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            x_s3 = F.relu(self.sparse_layer_tetrahedra(x_s2))\n",
    "\n",
    "            return torch.cat([x_s1, x_s2, x_s3], 1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return torch.cat([x_s1, x_s2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0cf9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutedMixingHNN(nn.Module):\n",
    "    @staticmethod\n",
    "    def get_connections_for_convoluted_mixing_hnn(\n",
    "        nodes_to_edges_connections: tuple,\n",
    "        num_convolutional_channels: int,\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        This function modifies the connections for the convoluted mixing HNN.\n",
    "        It expands the nodes_to_edges_connections to account for the convolutional channels.\n",
    "        \"\"\"\n",
    "        new_nodes_to_edges_connections = ([], [])\n",
    "        for connection_index in range(len(nodes_to_edges_connections[0])):\n",
    "            node_index = nodes_to_edges_connections[0][connection_index]\n",
    "            edge_index = nodes_to_edges_connections[1][connection_index]\n",
    "\n",
    "            for channel in range(num_convolutional_channels):\n",
    "                new_nodes_to_edges_connections[0].append(\n",
    "                    node_index * num_convolutional_channels + channel\n",
    "                )\n",
    "                new_nodes_to_edges_connections[1].append(edge_index)\n",
    "\n",
    "        return new_nodes_to_edges_connections\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        homological_structure: GraphHomologicalStructure,\n",
    "        num_convolutional_channels: int,\n",
    "        lighten: bool = False,\n",
    "    ):\n",
    "        super(ConvolutedMixingHNN, self).__init__()\n",
    "        self.name = \"hcnn\"\n",
    "        if lighten:\n",
    "            self.name += \"-lighten\"\n",
    "\n",
    "        self.homological_structure = homological_structure\n",
    "\n",
    "        self.conv_layer_price_vol = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=1,\n",
    "                out_channels=num_convolutional_channels,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        convoluted_nodes_to_edges_connections = (\n",
    "            self.get_connections_for_convoluted_mixing_hnn(\n",
    "                homological_structure.nodes_to_edges_connections,\n",
    "                num_convolutional_channels,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.convoluted_homological_structure = deepcopy(homological_structure)\n",
    "        self.convoluted_homological_structure.nodes_to_edges_connections = (\n",
    "            convoluted_nodes_to_edges_connections\n",
    "        )\n",
    "\n",
    "        self.hnn = HNN(self.convoluted_homological_structure)\n",
    "\n",
    "        self.readout_layer = nn.Linear(\n",
    "            in_features=homological_structure.num_edges\n",
    "            + homological_structure.num_triangles\n",
    "            + homological_structure.num_tetrahedra,\n",
    "            out_features=3,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  x.shape = (batch_size, 1, num_features) num_features è della dimensione di tutti i nodi (nodi nel senso di spazio-temporali, quindi vol1ask_lag0, vol1ask_lag1, ...) * 2 perche c'è price and volume\n",
    "\n",
    "        # after conv_layer_price_vol -> x.shape = (batch_size, num_convolutional_channels, num_features // 2)\n",
    "        x = self.conv_layer_price_vol(x)\n",
    "\n",
    "        # after flatten -> # x.shape = (batch_size, num_convolutional_channels * num_features // 2)\n",
    "        # Permute to have channels first, then flatten. so the columns will be feature_channel1, feature_channel2, ..., feature_channelN\n",
    "        x = x.permute(0, 2, 1).flatten(start_dim=1)\n",
    "\n",
    "        x = self.hnn(x)  # x.shape = (batch_size, num_classes)\n",
    "\n",
    "        # after hnn -> x.shape = (batch_size, num_edges + num_triangles + num_tetrahedra)\n",
    "        x = self.readout_layer(x)  # x.shape = (batch_size, num\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ac8a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 2, 1, 3, 1, 4, 2, 3, 2, 4, 3, 4],\n",
       " [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b35faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmhnn = ConvolutedMixingHNN(\n",
    "    homological_structure=GraphHomologicalStructure(\n",
    "        nodes_to_edges_connections=connection_1,\n",
    "        edges_to_triangles_connections=connection_2,\n",
    "        triangles_to_tetrahedra_connections=connection_3,\n",
    "    ),\n",
    "    num_convolutional_channels=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a39fa1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2990,  1.8206,  0.8336, -2.0264, -0.4700, -1.0977,  0.6162,\n",
       "           0.7628,  0.2390,  1.5506]],\n",
       "\n",
       "        [[ 1.5205, -0.3781,  1.6900,  1.0291,  0.2050,  0.5757,  0.8598,\n",
       "           0.5767,  1.0066,  0.7828]],\n",
       "\n",
       "        [[ 0.1221,  0.3215,  1.2858,  0.1554, -0.4019,  1.5746,  0.0791,\n",
       "          -0.0583,  1.1217, -0.2726]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FEATURES = G.number_of_nodes() * 2\n",
    "x = torch.randn(3, 1, N_FEATURES)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6425b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1222,  0.0877, -0.1908],\n",
      "        [-0.0963,  0.1122, -0.1353],\n",
      "        [-0.0119,  0.1761, -0.0889]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = cmhnn(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94d8bf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe93bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class CustomWindowedDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        windows_limits: list[tuple[int, int]],\n",
    "    ):\n",
    "        self.windows_limits = windows_limits\n",
    "        self.last_lag = windows_limits[-1][1]\n",
    "\n",
    "        self.df = pd.DataFrame(\n",
    "            data={f\"feature_{i}\": list(range(i, i + 100)) for i in range(5)},\n",
    "        )\n",
    "\n",
    "    def get_max_offset(self):\n",
    "        return self.last_lag\n",
    "\n",
    "    def get_window_data(self, cache_idx, start_idx):\n",
    "        columns_number = 5\n",
    "        window_means = [\n",
    "            self._get_single_window_mean(\n",
    "                start_idx, cache_idx, start_lag_window, end_lag_window, columns_number\n",
    "            )\n",
    "            for (start_lag_window, end_lag_window) in self.windows_limits\n",
    "        ]\n",
    "\n",
    "        # Stack into new DataFrame, reverse order (decreasing by window number)\n",
    "        result = pd.DataFrame(\n",
    "            window_means[::-1],\n",
    "            columns=self.df.columns[:columns_number],\n",
    "        )\n",
    "        result.index = range(len(result))  # reset index\n",
    "        return result\n",
    "\n",
    "    def _get_single_window_mean(\n",
    "        self,\n",
    "        start_idx: int,\n",
    "        cache_idx: int,\n",
    "        start_lag_window: int,\n",
    "        end_lag_window: int,\n",
    "        columns_number: int,\n",
    "    ) -> pd.Series:\n",
    "        start_window_idx = start_idx - end_lag_window + 1\n",
    "        end_window_idx = start_idx - start_lag_window + 1\n",
    "\n",
    "        window_df = self.df.iloc[start_window_idx:end_window_idx, :columns_number]\n",
    "\n",
    "        return window_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1ee2b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_0  feature_1  feature_2  feature_3  feature_4\n",
       "0           0          1          2          3          4\n",
       "1           1          2          3          4          5\n",
       "2           2          3          4          5          6\n",
       "3           3          4          5          6          7\n",
       "4           4          5          6          7          8\n",
       "..        ...        ...        ...        ...        ...\n",
       "95         95         96         97         98         99\n",
       "96         96         97         98         99        100\n",
       "97         97         98         99        100        101\n",
       "98         98         99        100        101        102\n",
       "99         99        100        101        102        103\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = CustomWindowedDataset(\n",
    "    windows_limits=[(0, 1), (1, 3), (3, 7), (7, 15), (15, 31)],\n",
    ")\n",
    "df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4122146b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>30.5</td>\n",
       "      <td>31.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.5</td>\n",
       "      <td>40.5</td>\n",
       "      <td>41.5</td>\n",
       "      <td>42.5</td>\n",
       "      <td>43.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.5</td>\n",
       "      <td>46.5</td>\n",
       "      <td>47.5</td>\n",
       "      <td>48.5</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.5</td>\n",
       "      <td>49.5</td>\n",
       "      <td>50.5</td>\n",
       "      <td>51.5</td>\n",
       "      <td>52.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4\n",
       "0       27.5       28.5       29.5       30.5       31.5\n",
       "1       39.5       40.5       41.5       42.5       43.5\n",
       "2       45.5       46.5       47.5       48.5       49.5\n",
       "3       48.5       49.5       50.5       51.5       52.5\n",
       "4       50.0       51.0       52.0       53.0       54.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.get_window_data(cache_idx=0, start_idx=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd3a5afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_0    27.5\n",
       "feature_1    28.5\n",
       "feature_2    29.5\n",
       "feature_3    30.5\n",
       "feature_4    31.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(0, 1), (1, 3), (3, 7), (7, 15), (15, 31)]\n",
    "\n",
    "df.df.iloc[20:36].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd7ae3",
   "metadata": {},
   "source": [
    "# DF CONSTRUCTION FOR STHNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b5f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import permutations, combinations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def separating_cliques(G):\n",
    "    clique_1 = []\n",
    "    clique_2 = []\n",
    "    clique_3 = []\n",
    "    clique_4 = []\n",
    "    for clique in nx.enumerate_all_cliques(G):\n",
    "        clique = set(clique)\n",
    "        if len(clique) == 1:\n",
    "            clique_1.append(clique)\n",
    "        elif len(clique) == 2:\n",
    "            clique_2.append(clique)\n",
    "        elif len(clique) == 3:\n",
    "            clique_3.append(clique)\n",
    "        elif len(clique) == 4:\n",
    "            clique_4.append(clique)\n",
    "    return clique_1, clique_2, clique_3, clique_4\n",
    "\n",
    "\n",
    "def get_connection(clique_last, clique_next):\n",
    "    connection_list = [[], []]\n",
    "    component_mapping = {i: x for i, x in enumerate(clique_last)}\n",
    "    for i, clique in enumerate(clique_next):\n",
    "        component = [set(x) for x in combinations(clique, len(clique) - 1)]\n",
    "        index_next = i\n",
    "        index_last = [\n",
    "            list(component_mapping.keys())[list(component_mapping.values()).index(x)]\n",
    "            for x in component\n",
    "        ]\n",
    "        for j in index_last:\n",
    "            connection_list[0].append(j)\n",
    "            connection_list[1].append(i)\n",
    "\n",
    "    return connection_list\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SparseLinear(nn.Module):\n",
    "    \"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        connectivity: user defined sparsity matrix\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        coalesce_device: device to coalesce the sparse matrix on\n",
    "            Default: 'gpu'\n",
    "        max_size (int): maximum number of entries allowed before chunking occurrs\n",
    "            Default: 1e8\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> m = nn.SparseLinear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        connectivity,\n",
    "        bias=True,\n",
    "        coalesce_device=\"cuda\",\n",
    "        max_size=1e8,\n",
    "    ):\n",
    "        assert in_features < 2**31 and out_features < 2**31\n",
    "        if connectivity is not None:\n",
    "            assert isinstance(connectivity, torch.LongTensor) or isinstance(\n",
    "                connectivity,\n",
    "                torch.cuda.LongTensor,\n",
    "            ), \"Connectivity must be a Long Tensor\"\n",
    "            assert (\n",
    "                connectivity.shape[0] == 2 and connectivity.shape[1] > 0\n",
    "            ), \"Input shape for connectivity should be (2,nnz)\"\n",
    "            assert (\n",
    "                connectivity.shape[1] <= in_features * out_features\n",
    "            ), \"Nnz can't be bigger than the weight matrix\"\n",
    "        super(SparseLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.connectivity = connectivity\n",
    "        self.max_size = max_size\n",
    "\n",
    "        nnz = connectivity.shape[1]\n",
    "        connectivity = connectivity.to(device=coalesce_device)\n",
    "        indices = connectivity\n",
    "\n",
    "        values = torch.empty(nnz, device=coalesce_device)\n",
    "\n",
    "        self.register_buffer(\"indices\", indices.cpu())\n",
    "        self.weights = nn.Parameter(values.cpu())\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / self.in_features**0.5\n",
    "        nn.init.uniform_(self.weights, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        \"\"\"returns a torch.sparse_coo_tensor view of the underlying weight matrix\n",
    "        This is only for inspection purposes and should not be modified or used in any autograd operations\n",
    "        \"\"\"\n",
    "        weight = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            (self.out_features, self.in_features),\n",
    "        )\n",
    "        return weight.coalesce().detach()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output_shape = list(inputs.shape)\n",
    "        output_shape[-1] = self.out_features\n",
    "\n",
    "        if len(output_shape) == 1:\n",
    "            inputs = inputs.view(1, -1)\n",
    "        inputs = inputs.flatten(end_dim=-2)\n",
    "\n",
    "        target = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            torch.Size([self.out_features, self.in_features]),\n",
    "        )\n",
    "        output = torch.sparse.mm(target, inputs.t()).t()\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output.view(output_shape)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"in_features={}, out_features={}, bias={}, connectivity={}\".format(\n",
    "            self.in_features,\n",
    "            self.out_features,\n",
    "            self.bias is not None,\n",
    "            self.connectivity,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GraphHomologicalStructure:\n",
    "    nodes_to_edges_connections: tuple\n",
    "    edges_to_triangles_connections: tuple\n",
    "    triangles_to_tetrahedra_connections: tuple\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self) -> int:\n",
    "        return max(self.nodes_to_edges_connections[0]) + 1\n",
    "\n",
    "    @property\n",
    "    def num_edges(self) -> int:\n",
    "        return max(self.edges_to_triangles_connections[0]) + 1\n",
    "\n",
    "    @property\n",
    "    def num_triangles(self) -> int:\n",
    "        return (\n",
    "            max(self.triangles_to_tetrahedra_connections[0]) + 1\n",
    "            if self.triangles_to_tetrahedra_connections\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_tetrahedra(self) -> int:\n",
    "        return (\n",
    "            max(self.triangles_to_tetrahedra_connections[1]) + 1\n",
    "            if self.triangles_to_tetrahedra_connections\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    def get_nodes_to_edges_connections_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                self.nodes_to_edges_connections[1],\n",
    "                self.nodes_to_edges_connections[0],\n",
    "            ],\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "\n",
    "    def get_edges_to_triangles_connections_tensor(self) -> torch.Tensor:\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                self.edges_to_triangles_connections[1],\n",
    "                self.edges_to_triangles_connections[0],\n",
    "            ],\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "\n",
    "    def get_triangles_to_tetrahedra_connections_tensor(self) -> torch.Tensor:\n",
    "        return (\n",
    "            torch.tensor(\n",
    "                [\n",
    "                    self.triangles_to_tetrahedra_connections[1],\n",
    "                    self.triangles_to_tetrahedra_connections[0],\n",
    "                ],\n",
    "                dtype=torch.int64,\n",
    "            )\n",
    "            if self.triangles_to_tetrahedra_connections\n",
    "            else torch.empty((2, 0), dtype=torch.int64)\n",
    "        )\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        return GraphHomologicalStructure(\n",
    "            nodes_to_edges_connections=deepcopy(self.nodes_to_edges_connections, memo),\n",
    "            edges_to_triangles_connections=deepcopy(\n",
    "                self.edges_to_triangles_connections, memo\n",
    "            ),\n",
    "            triangles_to_tetrahedra_connections=deepcopy(\n",
    "                self.triangles_to_tetrahedra_connections, memo\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class HNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        homological_structure: GraphHomologicalStructure,\n",
    "    ):\n",
    "        super(HNN, self).__init__()\n",
    "        self.homological_structure = homological_structure\n",
    "\n",
    "        self.sparse_layer_edges = SparseLinear(\n",
    "            homological_structure.num_nodes,\n",
    "            homological_structure.num_edges,\n",
    "            connectivity=self.homological_structure.get_nodes_to_edges_connections_tensor(),\n",
    "        )\n",
    "\n",
    "        self.sparse_layer_triangles = SparseLinear(\n",
    "            self.homological_structure.num_edges,\n",
    "            self.homological_structure.num_triangles,\n",
    "            connectivity=self.homological_structure.get_edges_to_triangles_connections_tensor(),\n",
    "        )\n",
    "\n",
    "        if len(self.homological_structure.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            self.sparse_layer_tetrahedra = SparseLinear(\n",
    "                self.homological_structure.num_triangles,\n",
    "                self.homological_structure.num_tetrahedra,\n",
    "                connectivity=self.homological_structure.get_triangles_to_tetrahedra_connections_tensor(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.sparse_layer_tetrahedra = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s1 = F.relu(self.sparse_layer_edges(x))\n",
    "\n",
    "        x_s2 = F.relu(self.sparse_layer_triangles(x_s1))\n",
    "\n",
    "        if len(self.homological_structure.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            x_s3 = F.relu(self.sparse_layer_tetrahedra(x_s2))\n",
    "\n",
    "            return torch.cat([x_s1, x_s2, x_s3], 1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return torch.cat([x_s1, x_s2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6803fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutedMixingHNN(nn.Module):\n",
    "    @staticmethod\n",
    "    def get_connections_for_convoluted_mixing_hnn(\n",
    "        nodes_to_edges_connections: tuple,\n",
    "        num_convolutional_channels: int,\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        This function modifies the connections for the convoluted mixing HNN.\n",
    "        It expands the nodes_to_edges_connections to account for the convolutional channels.\n",
    "        \"\"\"\n",
    "        new_nodes_to_edges_connections = ([], [])\n",
    "        for connection_index in range(len(nodes_to_edges_connections[0])):\n",
    "            node_index = nodes_to_edges_connections[0][connection_index]\n",
    "            edge_index = nodes_to_edges_connections[1][connection_index]\n",
    "\n",
    "            for channel in range(num_convolutional_channels):\n",
    "                new_nodes_to_edges_connections[0].append(\n",
    "                    node_index * num_convolutional_channels + channel\n",
    "                )\n",
    "                new_nodes_to_edges_connections[1].append(edge_index)\n",
    "\n",
    "        return new_nodes_to_edges_connections\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        homological_structure: GraphHomologicalStructure,\n",
    "        num_convolutional_channels: int,\n",
    "        lighten: bool = False,\n",
    "    ):\n",
    "        super(ConvolutedMixingHNN, self).__init__()\n",
    "        self.name = \"hcnn\"\n",
    "        if lighten:\n",
    "            self.name += \"-lighten\"\n",
    "\n",
    "        self.homological_structure = homological_structure\n",
    "\n",
    "        self.conv_layer_price_vol = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=1,\n",
    "                out_channels=num_convolutional_channels,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        convoluted_nodes_to_edges_connections = (\n",
    "            self.get_connections_for_convoluted_mixing_hnn(\n",
    "                homological_structure.nodes_to_edges_connections,\n",
    "                num_convolutional_channels,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.convoluted_homological_structure = deepcopy(homological_structure)\n",
    "        self.convoluted_homological_structure.nodes_to_edges_connections = (\n",
    "            convoluted_nodes_to_edges_connections\n",
    "        )\n",
    "\n",
    "        self.hnn = HNN(self.convoluted_homological_structure)\n",
    "\n",
    "        self.readout_layer = nn.Linear(\n",
    "            in_features=homological_structure.num_edges\n",
    "            + homological_structure.num_triangles\n",
    "            + homological_structure.num_tetrahedra,\n",
    "            out_features=3,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch_size, 1, num_window_lags, num_spatial_features) num_spatial_features è della dimensione di tutti i nodi (nodi nel senso di spaziali quindi senza lag) * 2 perche c'è price and volume\n",
    "\n",
    "        # After these -> x.shape = (batch_size, 1, num_features) num_features è della dimensione di tutti i nodi (nodi nel senso di spazio-temporali, quindi vol1ask_lag0, vol1ask_lag1, ...) * 2 perche c'è price and volume\n",
    "        x = torch.flip(x, dims=[2])\n",
    "        x = x.reshape(x.shape[0], 1, -1)\n",
    "\n",
    "        # after conv_layer_price_vol -> x.shape = (batch_size, num_convolutional_channels, num_features // 2)\n",
    "        x = self.conv_layer_price_vol(x)\n",
    "\n",
    "        # after flatten -> # x.shape = (batch_size, num_convolutional_channels * num_features // 2)\n",
    "        # Permute to have channels first, then flatten. so the columns will be feature_channel1, feature_channel2, ..., feature_channelN\n",
    "        x = x.permute(0, 2, 1).flatten(start_dim=1)\n",
    "\n",
    "        x = self.hnn(x)  # x.shape = (batch_size, num_classes)\n",
    "\n",
    "        # after hnn -> x.shape = (batch_size, num_edges + num_triangles + num_tetrahedra)\n",
    "        x = self.readout_layer(x)  # x.shape = (batch_size, num\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55b9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from loaders.custom_dataset import CustomDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomWindowedDataset(CustomDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        learning_stage,\n",
    "        windows_limits: list[tuple[int, int]],\n",
    "        shuffling_seed,\n",
    "        cache_size,\n",
    "        lighten,\n",
    "        threshold,\n",
    "        all_horizons,\n",
    "        prediction_horizon,\n",
    "        targets_type,\n",
    "        balanced_dataloader=False,\n",
    "        backtest=False,\n",
    "        training_stocks=None,\n",
    "        validation_stocks=None,\n",
    "        target_stocks=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        windows_limits is a list of tuples specifying the start and end indices for each window.\n",
    "        It must be ordered. End index must be excluded. If I have [(0,2), (2,5), (5,10)], it means I have three windows:\n",
    "        - Window 1: 0,1\n",
    "        - Window 2: 2,3,4\n",
    "        - Window 3: 5,6,7,8,9\n",
    "        \"\"\"\n",
    "        self.windows_limits = windows_limits\n",
    "        self.last_lag = windows_limits[-1][1]\n",
    "\n",
    "        super().__init__(\n",
    "            dataset=dataset,\n",
    "            learning_stage=learning_stage,\n",
    "            shuffling_seed=shuffling_seed,\n",
    "            cache_size=cache_size,\n",
    "            lighten=lighten,\n",
    "            threshold=threshold,\n",
    "            all_horizons=all_horizons,\n",
    "            prediction_horizon=prediction_horizon,\n",
    "            targets_type=targets_type,\n",
    "            balanced_dataloader=balanced_dataloader,\n",
    "            backtest=backtest,\n",
    "            training_stocks=training_stocks,\n",
    "            validation_stocks=validation_stocks,\n",
    "            target_stocks=target_stocks,\n",
    "        )\n",
    "\n",
    "    def get_max_offset(self):\n",
    "        return self.last_lag\n",
    "\n",
    "    def get_window_data(self, cache_idx, start_idx):\n",
    "        columns_number = 20 if self.lighten else 40\n",
    "        window_means = [\n",
    "            self._get_single_window_mean(\n",
    "                start_idx, cache_idx, start_lag_window, end_lag_window, columns_number\n",
    "            )\n",
    "            for (start_lag_window, end_lag_window) in self.windows_limits\n",
    "        ]\n",
    "\n",
    "        # Stack into new DataFrame, reverse order (decreasing by window number)\n",
    "        result = np.vstack(window_means[::-1])\n",
    "        return result\n",
    "\n",
    "    def _get_single_window_mean(\n",
    "        self,\n",
    "        start_idx: int,\n",
    "        cache_idx: int,\n",
    "        start_lag_window: int,\n",
    "        end_lag_window: int,\n",
    "        columns_number: int,\n",
    "    ) -> pd.Series:\n",
    "        # + 1 in necessary; otherwise the window will be shifted by one in the past\n",
    "        start_window_idx = start_idx - end_lag_window + 1\n",
    "        end_window_idx = start_idx - start_lag_window + 1\n",
    "\n",
    "        window_df = self.cache_data[cache_idx][\n",
    "            start_window_idx:end_window_idx, :columns_number\n",
    "        ]\n",
    "\n",
    "        return window_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69ec1151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n",
      "/tmp/ipykernel_42193/3540425175.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag1\"] = df[col].shift(1)\n",
      "/tmp/ipykernel_42193/3540425175.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag2\"] = df[col].shift(2)\n",
      "/tmp/ipykernel_42193/3540425175.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag3\"] = df[col].shift(3)\n",
      "/tmp/ipykernel_42193/3540425175.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_lag4\"] = df[col].shift(4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASKs1</th>\n",
       "      <th>ASKp1</th>\n",
       "      <th>BIDs1</th>\n",
       "      <th>BIDp1</th>\n",
       "      <th>ASKs2</th>\n",
       "      <th>ASKp2</th>\n",
       "      <th>BIDs2</th>\n",
       "      <th>BIDp2</th>\n",
       "      <th>ASKs3</th>\n",
       "      <th>ASKp3</th>\n",
       "      <th>...</th>\n",
       "      <th>BIDs8_lag4</th>\n",
       "      <th>BIDp8_lag4</th>\n",
       "      <th>ASKs9_lag4</th>\n",
       "      <th>ASKp9_lag4</th>\n",
       "      <th>BIDs9_lag4</th>\n",
       "      <th>BIDp9_lag4</th>\n",
       "      <th>ASKs10_lag4</th>\n",
       "      <th>ASKp10_lag4</th>\n",
       "      <th>BIDs10_lag4</th>\n",
       "      <th>BIDp10_lag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1600</td>\n",
       "      <td>571300</td>\n",
       "      <td>760</td>\n",
       "      <td>571200</td>\n",
       "      <td>2018</td>\n",
       "      <td>571400</td>\n",
       "      <td>2126</td>\n",
       "      <td>571100</td>\n",
       "      <td>1250</td>\n",
       "      <td>571500</td>\n",
       "      <td>...</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>570500.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>572100.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>570400.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>572200.0</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>570300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>571300</td>\n",
       "      <td>560</td>\n",
       "      <td>571200</td>\n",
       "      <td>2018</td>\n",
       "      <td>571400</td>\n",
       "      <td>2126</td>\n",
       "      <td>571100</td>\n",
       "      <td>1250</td>\n",
       "      <td>571500</td>\n",
       "      <td>...</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>570500.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>572100.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>570400.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>572200.0</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>570300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1600</td>\n",
       "      <td>571300</td>\n",
       "      <td>560</td>\n",
       "      <td>571200</td>\n",
       "      <td>2018</td>\n",
       "      <td>571400</td>\n",
       "      <td>2026</td>\n",
       "      <td>571100</td>\n",
       "      <td>1250</td>\n",
       "      <td>571500</td>\n",
       "      <td>...</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>570500.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>572100.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>570400.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>572200.0</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>570300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1600</td>\n",
       "      <td>571300</td>\n",
       "      <td>560</td>\n",
       "      <td>571200</td>\n",
       "      <td>2018</td>\n",
       "      <td>571400</td>\n",
       "      <td>2026</td>\n",
       "      <td>571100</td>\n",
       "      <td>1250</td>\n",
       "      <td>571500</td>\n",
       "      <td>...</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>570500.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>572100.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>570400.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>572200.0</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>570300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1600</td>\n",
       "      <td>571300</td>\n",
       "      <td>560</td>\n",
       "      <td>571200</td>\n",
       "      <td>2018</td>\n",
       "      <td>571400</td>\n",
       "      <td>1926</td>\n",
       "      <td>571100</td>\n",
       "      <td>1250</td>\n",
       "      <td>571500</td>\n",
       "      <td>...</td>\n",
       "      <td>1643.0</td>\n",
       "      <td>570500.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>572100.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>570400.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>572200.0</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>570300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>1215</td>\n",
       "      <td>571100</td>\n",
       "      <td>1801</td>\n",
       "      <td>571000</td>\n",
       "      <td>2400</td>\n",
       "      <td>571200</td>\n",
       "      <td>2800</td>\n",
       "      <td>570900</td>\n",
       "      <td>2000</td>\n",
       "      <td>571300</td>\n",
       "      <td>...</td>\n",
       "      <td>1462.0</td>\n",
       "      <td>570300.0</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>571900.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>570200.0</td>\n",
       "      <td>2355.0</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>570100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>1215</td>\n",
       "      <td>571100</td>\n",
       "      <td>1801</td>\n",
       "      <td>571000</td>\n",
       "      <td>2400</td>\n",
       "      <td>571200</td>\n",
       "      <td>2800</td>\n",
       "      <td>570900</td>\n",
       "      <td>2100</td>\n",
       "      <td>571300</td>\n",
       "      <td>...</td>\n",
       "      <td>1462.0</td>\n",
       "      <td>570300.0</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>571900.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>570200.0</td>\n",
       "      <td>2355.0</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>570100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>1215</td>\n",
       "      <td>571100</td>\n",
       "      <td>1801</td>\n",
       "      <td>571000</td>\n",
       "      <td>2400</td>\n",
       "      <td>571200</td>\n",
       "      <td>2800</td>\n",
       "      <td>570900</td>\n",
       "      <td>2100</td>\n",
       "      <td>571300</td>\n",
       "      <td>...</td>\n",
       "      <td>1462.0</td>\n",
       "      <td>570300.0</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>571900.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>570200.0</td>\n",
       "      <td>2355.0</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>570100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>1215</td>\n",
       "      <td>571100</td>\n",
       "      <td>2581</td>\n",
       "      <td>571000</td>\n",
       "      <td>2400</td>\n",
       "      <td>571200</td>\n",
       "      <td>2800</td>\n",
       "      <td>570900</td>\n",
       "      <td>2100</td>\n",
       "      <td>571300</td>\n",
       "      <td>...</td>\n",
       "      <td>1462.0</td>\n",
       "      <td>570300.0</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>571900.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>570200.0</td>\n",
       "      <td>2355.0</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>570100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1215</td>\n",
       "      <td>571100</td>\n",
       "      <td>2581</td>\n",
       "      <td>571000</td>\n",
       "      <td>2400</td>\n",
       "      <td>571200</td>\n",
       "      <td>2800</td>\n",
       "      <td>570900</td>\n",
       "      <td>2100</td>\n",
       "      <td>571300</td>\n",
       "      <td>...</td>\n",
       "      <td>1462.0</td>\n",
       "      <td>570300.0</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>571900.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>570200.0</td>\n",
       "      <td>2355.0</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>570100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ASKs1   ASKp1  BIDs1   BIDp1  ASKs2   ASKp2  BIDs2   BIDp2  ASKs3  \\\n",
       "0     1600  571300    760  571200   2018  571400   2126  571100   1250   \n",
       "1     1600  571300    560  571200   2018  571400   2126  571100   1250   \n",
       "2     1600  571300    560  571200   2018  571400   2026  571100   1250   \n",
       "3     1600  571300    560  571200   2018  571400   2026  571100   1250   \n",
       "4     1600  571300    560  571200   2018  571400   1926  571100   1250   \n",
       "..     ...     ...    ...     ...    ...     ...    ...     ...    ...   \n",
       "991   1215  571100   1801  571000   2400  571200   2800  570900   2000   \n",
       "992   1215  571100   1801  571000   2400  571200   2800  570900   2100   \n",
       "993   1215  571100   1801  571000   2400  571200   2800  570900   2100   \n",
       "994   1215  571100   2581  571000   2400  571200   2800  570900   2100   \n",
       "995   1215  571100   2581  571000   2400  571200   2800  570900   2100   \n",
       "\n",
       "      ASKp3  ...  BIDs8_lag4  BIDp8_lag4  ASKs9_lag4  ASKp9_lag4  BIDs9_lag4  \\\n",
       "0    571500  ...      1643.0    570500.0       801.0    572100.0      2758.0   \n",
       "1    571500  ...      1643.0    570500.0       801.0    572100.0      2758.0   \n",
       "2    571500  ...      1643.0    570500.0       801.0    572100.0      2758.0   \n",
       "3    571500  ...      1643.0    570500.0       801.0    572100.0      2758.0   \n",
       "4    571500  ...      1643.0    570500.0       801.0    572100.0      2758.0   \n",
       "..      ...  ...         ...         ...         ...         ...         ...   \n",
       "991  571300  ...      1462.0    570300.0      1575.0    571900.0      2020.0   \n",
       "992  571300  ...      1462.0    570300.0      1575.0    571900.0      2020.0   \n",
       "993  571300  ...      1462.0    570300.0      1575.0    571900.0      2020.0   \n",
       "994  571300  ...      1462.0    570300.0      1575.0    571900.0      2020.0   \n",
       "995  571300  ...      1462.0    570300.0      1575.0    571900.0      2020.0   \n",
       "\n",
       "     BIDp9_lag4  ASKs10_lag4  ASKp10_lag4  BIDs10_lag4  BIDp10_lag4  \n",
       "0      570400.0       1501.0     572200.0       1362.0     570300.0  \n",
       "1      570400.0       1501.0     572200.0       1362.0     570300.0  \n",
       "2      570400.0       1501.0     572200.0       1362.0     570300.0  \n",
       "3      570400.0       1501.0     572200.0       1362.0     570300.0  \n",
       "4      570400.0       1501.0     572200.0       1362.0     570300.0  \n",
       "..          ...          ...          ...          ...          ...  \n",
       "991    570200.0       2355.0     572000.0       1100.0     570100.0  \n",
       "992    570200.0       2355.0     572000.0       1100.0     570100.0  \n",
       "993    570200.0       2355.0     572000.0       1100.0     570100.0  \n",
       "994    570200.0       2355.0     572000.0       1100.0     570100.0  \n",
       "995    570200.0       2355.0     572000.0       1100.0     570100.0  \n",
       "\n",
       "[996 rows x 200 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "from itertools import permutations, combinations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from fast_tmfg import TMFG\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/home/daxus/code/LOBFrame/data/nasdaq/unscaled_data/CSCO/CSCO_orderbooks_2019-06-11.csv\"\n",
    ")\n",
    "df = df.drop(\n",
    "    columns=[\n",
    "        \"seconds\",\n",
    "        \"Raw_Target_10\",\n",
    "        \"Raw_Target_50\",\n",
    "        \"Raw_Target_100\",\n",
    "        \"Smooth_Target_10\",\n",
    "        \"Smooth_Target_50\",\n",
    "        \"Smooth_Target_100\",\n",
    "    ]\n",
    ")\n",
    "df = df.iloc[:1000].copy()\n",
    "\n",
    "for col in df.columns:\n",
    "    df[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "    df[f\"{col}_lag2\"] = df[col].shift(2)\n",
    "    df[f\"{col}_lag3\"] = df[col].shift(3)\n",
    "    df[f\"{col}_lag4\"] = df[col].shift(4)\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def parse_col(c):\n",
    "    lag = re.search(r\"_lag(\\d+)\", c)\n",
    "    lag_num = int(lag.group(1)) if lag else 0\n",
    "    c_clean = c.split(\"_lag\")[0]\n",
    "    match = re.match(r\"(ASKs|ASKp|BIDs|BIDp)(\\d+)\", c_clean)\n",
    "    if match:\n",
    "        prefix, level = match.groups()\n",
    "        return prefix, int(level), lag_num\n",
    "    return c, 0, lag_num\n",
    "\n",
    "\n",
    "def sort_key(c):\n",
    "    if \"Target\" in c:\n",
    "        return (99, 99, 99)  # Put target columns at the end\n",
    "\n",
    "    order = [\"ASKs\", \"ASKp\", \"BIDs\", \"BIDp\"]\n",
    "    prefix, level, lag_num = parse_col(c)\n",
    "    return (\n",
    "        lag_num,  # non-lagged first\n",
    "        level,  # by level number\n",
    "        order.index(prefix) if prefix in order else 99,  # by prefix order\n",
    "    )\n",
    "\n",
    "\n",
    "df = df[sorted(df.columns, key=sort_key)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8439f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASKs1</th>\n",
       "      <th>BIDs1</th>\n",
       "      <th>ASKs2</th>\n",
       "      <th>BIDs2</th>\n",
       "      <th>ASKs3</th>\n",
       "      <th>BIDs3</th>\n",
       "      <th>ASKs4</th>\n",
       "      <th>BIDs4</th>\n",
       "      <th>ASKs5</th>\n",
       "      <th>BIDs5</th>\n",
       "      <th>...</th>\n",
       "      <th>ASKs6_lag4</th>\n",
       "      <th>BIDs6_lag4</th>\n",
       "      <th>ASKs7_lag4</th>\n",
       "      <th>BIDs7_lag4</th>\n",
       "      <th>ASKs8_lag4</th>\n",
       "      <th>BIDs8_lag4</th>\n",
       "      <th>ASKs9_lag4</th>\n",
       "      <th>BIDs9_lag4</th>\n",
       "      <th>ASKs10_lag4</th>\n",
       "      <th>BIDs10_lag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ASKs1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIDs1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASKs2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIDs2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASKs3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIDs8_lag4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.981974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASKs9_lag4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIDs9_lag4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.974709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASKs10_lag4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIDs10_lag4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.981974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ASKs1  BIDs1  ASKs2  BIDs2  ASKs3  BIDs3  ASKs4  BIDs4  ASKs5  \\\n",
       "ASKs1          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "BIDs1          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ASKs2          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "BIDs2          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ASKs3          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...            ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "BIDs8_lag4     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ASKs9_lag4     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "BIDs9_lag4     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ASKs10_lag4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "BIDs10_lag4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "             BIDs5  ...  ASKs6_lag4  BIDs6_lag4  ASKs7_lag4  BIDs7_lag4  \\\n",
       "ASKs1          0.0  ...         0.0         0.0         0.0    0.000000   \n",
       "BIDs1          0.0  ...         0.0         0.0         0.0    0.000000   \n",
       "ASKs2          0.0  ...         0.0         0.0         0.0    0.000000   \n",
       "BIDs2          0.0  ...         0.0         0.0         0.0    0.000000   \n",
       "ASKs3          0.0  ...         0.0         0.0         0.0    0.000000   \n",
       "...            ...  ...         ...         ...         ...         ...   \n",
       "BIDs8_lag4     0.0  ...         0.0         0.0         0.0    0.000000   \n",
       "ASKs9_lag4     0.0  ...         0.0         0.0         0.0    0.000000   \n",
       "BIDs9_lag4     0.0  ...         0.0         0.0         0.0    0.974709   \n",
       "ASKs10_lag4    0.0  ...         0.0         0.0         0.0    0.000000   \n",
       "BIDs10_lag4    0.0  ...         0.0         0.0         0.0    0.000000   \n",
       "\n",
       "             ASKs8_lag4  BIDs8_lag4  ASKs9_lag4  BIDs9_lag4  ASKs10_lag4  \\\n",
       "ASKs1               0.0    0.000000         0.0         0.0          0.0   \n",
       "BIDs1               0.0    0.000000         0.0         0.0          0.0   \n",
       "ASKs2               0.0    0.000000         0.0         0.0          0.0   \n",
       "BIDs2               0.0    0.000000         0.0         0.0          0.0   \n",
       "ASKs3               0.0    0.000000         0.0         0.0          0.0   \n",
       "...                 ...         ...         ...         ...          ...   \n",
       "BIDs8_lag4          0.0    0.000000         0.0         0.0          0.0   \n",
       "ASKs9_lag4          0.0    0.000000         0.0         0.0          0.0   \n",
       "BIDs9_lag4          0.0    0.000000         0.0         0.0          0.0   \n",
       "ASKs10_lag4         0.0    0.000000         0.0         0.0          0.0   \n",
       "BIDs10_lag4         0.0    0.981974         0.0         0.0          0.0   \n",
       "\n",
       "             BIDs10_lag4  \n",
       "ASKs1           0.000000  \n",
       "BIDs1           0.000000  \n",
       "ASKs2           0.000000  \n",
       "BIDs2           0.000000  \n",
       "ASKs3           0.000000  \n",
       "...                  ...  \n",
       "BIDs8_lag4      0.981974  \n",
       "ASKs9_lag4      0.000000  \n",
       "BIDs9_lag4      0.000000  \n",
       "ASKs10_lag4     0.000000  \n",
       "BIDs10_lag4     0.000000  \n",
       "\n",
       "[100 rows x 100 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = df[[col for col in df.columns if \"p\" not in col]].corr()\n",
    "model_all = TMFG()\n",
    "cliques_all, seps_all, adj_matrix_all = model_all.fit_transform(\n",
    "    corr_matrix, output=\"weighted_sparse_W_matrix\"\n",
    ")\n",
    "adj_matrix_all_df = pd.DataFrame(\n",
    "    adj_matrix_all, index=corr_matrix.index, columns=corr_matrix.columns\n",
    ")\n",
    "adj_matrix_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12ce416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_array(adj_matrix_all)\n",
    "clique_1, clique_2, clique_3, clique_4 = separating_cliques(G)\n",
    "\n",
    "connection_1 = get_connection(clique_1, clique_2)\n",
    "connection_2 = get_connection(clique_2, clique_3)\n",
    "connection_3 = get_connection(clique_3, clique_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "618880b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNBALANCED dataset construction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now i try to build the HNN\n",
    "# df = pd.read_csv(\n",
    "#     \"/home/daxus/code/LOBFrame/data/nasdaq/unscaled_data/CSCO/CSCO_orderbooks_2019-06-11.csv\"\n",
    "# )\n",
    "# df = df.drop(\n",
    "#     columns=[\n",
    "#         \"seconds\",\n",
    "#         \"Raw_Target_10\",\n",
    "#         \"Raw_Target_50\",\n",
    "#         \"Raw_Target_100\",\n",
    "#         \"Smooth_Target_10\",\n",
    "#         \"Smooth_Target_100\",\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# df = df[\n",
    "#     [\n",
    "#         col\n",
    "#         for col in df.columns\n",
    "#         if \"6\" not in col\n",
    "#         and \"10\" not in col\n",
    "#         and \"7\" not in col\n",
    "#         and \"8\" not in col\n",
    "#         and \"9\" not in col\n",
    "#     ]\n",
    "# ]\n",
    "# df = df.iloc[:1000].copy()\n",
    "# df = df[sorted(df.columns, key=sort_key)]\n",
    "\n",
    "\n",
    "hnn = ConvolutedMixingHNN(\n",
    "    homological_structure=GraphHomologicalStructure(\n",
    "        nodes_to_edges_connections=connection_1,\n",
    "        edges_to_triangles_connections=connection_2,\n",
    "        triangles_to_tetrahedra_connections=connection_3,\n",
    "    ),\n",
    "    num_convolutional_channels=4,\n",
    ")\n",
    "\n",
    "dataset = CustomWindowedDataset(\n",
    "    dataset=\"nasdaq\",\n",
    "    learning_stage=\"training\",\n",
    "    windows_limits=[(0, 1), (1, 3), (3, 7), (7, 15), (15, 31)],\n",
    "    shuffling_seed=42,\n",
    "    cache_size=1,\n",
    "    lighten=False,\n",
    "    threshold=32,\n",
    "    targets_type=\"raw\",\n",
    "    all_horizons=[5, 10, 30, 50, 100],\n",
    "    prediction_horizon=100,\n",
    "    balanced_dataloader=False,\n",
    "    training_stocks=[\"CSCO\"],\n",
    "    validation_stocks=[\"CSCO\"],\n",
    "    target_stocks=[\"CSCO\"],\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    "    sampler=dataset.glob_indices,\n",
    ")\n",
    "\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c6b6f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_list = []\n",
    "# Example usage of the DataLoader\n",
    "batch_data, batch_labels = next(iter(dataloader))\n",
    "\n",
    "out = hnn.forward(batch_data)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lobframe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
