{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51d9b864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f10e5fefa30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from itertools import permutations, combinations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SparseLinear(nn.Module):\n",
    "    \"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        connectivity: user defined sparsity matrix\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "        coalesce_device: device to coalesce the sparse matrix on\n",
    "            Default: 'gpu'\n",
    "        max_size (int): maximum number of entries allowed before chunking occurrs\n",
    "            Default: 1e8\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        >>> m = nn.SparseLinear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        connectivity,\n",
    "        bias=True,\n",
    "        coalesce_device=\"cuda\",\n",
    "        max_size=1e8,\n",
    "    ):\n",
    "        assert in_features < 2**31 and out_features < 2**31\n",
    "        if connectivity is not None:\n",
    "            assert isinstance(connectivity, torch.LongTensor) or isinstance(\n",
    "                connectivity,\n",
    "                torch.cuda.LongTensor,\n",
    "            ), \"Connectivity must be a Long Tensor\"\n",
    "            assert (\n",
    "                connectivity.shape[0] == 2 and connectivity.shape[1] > 0\n",
    "            ), \"Input shape for connectivity should be (2,nnz)\"\n",
    "            assert (\n",
    "                connectivity.shape[1] <= in_features * out_features\n",
    "            ), \"Nnz can't be bigger than the weight matrix\"\n",
    "        super(SparseLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.connectivity = connectivity\n",
    "        self.max_size = max_size\n",
    "\n",
    "        nnz = connectivity.shape[1]\n",
    "        connectivity = connectivity.to(device=coalesce_device)\n",
    "        indices = connectivity\n",
    "\n",
    "        values = torch.empty(nnz, device=coalesce_device)\n",
    "\n",
    "        self.register_buffer(\"indices\", indices.cpu())\n",
    "        self.weights = nn.Parameter(values.cpu())\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        bound = 1 / self.in_features**0.5\n",
    "        nn.init.uniform_(self.weights, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        \"\"\"returns a torch.sparse_coo_tensor view of the underlying weight matrix\n",
    "        This is only for inspection purposes and should not be modified or used in any autograd operations\n",
    "        \"\"\"\n",
    "        weight = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            (self.out_features, self.in_features),\n",
    "        )\n",
    "        return weight.coalesce().detach()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output_shape = list(inputs.shape)\n",
    "        output_shape[-1] = self.out_features\n",
    "\n",
    "        if len(output_shape) == 1:\n",
    "            inputs = inputs.view(1, -1)\n",
    "        inputs = inputs.flatten(end_dim=-2)\n",
    "\n",
    "        target = torch.sparse_coo_tensor(\n",
    "            self.indices,\n",
    "            self.weights,\n",
    "            torch.Size([self.out_features, self.in_features]),\n",
    "        )\n",
    "        output = torch.sparse.mm(target, inputs.t()).t()\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "\n",
    "        return output.view(output_shape)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"in_features={}, out_features={}, bias={}, connectivity={}\".format(\n",
    "            self.in_features,\n",
    "            self.out_features,\n",
    "            self.bias is not None,\n",
    "            self.connectivity,\n",
    "        )\n",
    "\n",
    "\n",
    "def separating_cliques(G):\n",
    "    clique_1 = []\n",
    "    clique_2 = []\n",
    "    clique_3 = []\n",
    "    clique_4 = []\n",
    "    for clique in nx.enumerate_all_cliques(G):\n",
    "        clique = set(clique)\n",
    "        if len(clique) == 1:\n",
    "            clique_1.append(clique)\n",
    "        elif len(clique) == 2:\n",
    "            clique_2.append(clique)\n",
    "        elif len(clique) == 3:\n",
    "            clique_3.append(clique)\n",
    "        elif len(clique) == 4:\n",
    "            clique_4.append(clique)\n",
    "    return clique_1, clique_2, clique_3, clique_4\n",
    "\n",
    "\n",
    "def get_connection(clique_last, clique_next):\n",
    "    connection_list = [[], []]\n",
    "    component_mapping = {i: x for i, x in enumerate(clique_last)}\n",
    "    for i, clique in enumerate(clique_next):\n",
    "        component = [set(x) for x in combinations(clique, len(clique) - 1)]\n",
    "        index_next = i\n",
    "        index_last = [\n",
    "            list(component_mapping.keys())[list(component_mapping.values()).index(x)]\n",
    "            for x in component\n",
    "        ]\n",
    "        for j in index_last:\n",
    "            connection_list[0].append(j)\n",
    "            connection_list[1].append(i)\n",
    "\n",
    "    return connection_list\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "# Add 4 nodes\n",
    "G.add_nodes_from([1, 2, 3, 4, 5])\n",
    "# Add 4 edges\n",
    "G.add_edges_from([(1, 2), (2, 3), (2, 4), (3, 4), (4, 5), (3, 5), (2, 5)])\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8826d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 4, 1, 3, 5, 2, 3, 6, 4, 5, 6], [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clique_1, clique_2, clique_3, clique_4 = separating_cliques(G)\n",
    "\n",
    "connection_1 = get_connection(clique_1, clique_2)\n",
    "connection_2 = get_connection(clique_2, clique_3)\n",
    "connection_3 = get_connection(clique_3, clique_4)\n",
    "\n",
    "connection_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e14903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6980, -0.3525, -0.5858, -0.2744, -0.2064,  0.2922, -0.3019]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "len_input = len(np.unique(connection_1[0]))\n",
    "len_output = len(np.unique(connection_1[1]))\n",
    "\n",
    "sl = SparseLinear(\n",
    "    in_features=len_input,\n",
    "    out_features=len_output,\n",
    "    connectivity=torch.tensor([connection_1[1], connection_1[0]], dtype=torch.int64),\n",
    ")\n",
    "x = torch.ones(1, len_input)\n",
    "output = sl(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e3dc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06015173],\n",
       "       [ 0.5625891 ],\n",
       "       [-0.09905863],\n",
       "       [ 0.12613186],\n",
       "       [-0.22061317],\n",
       "       [ 0.6008882 ],\n",
       "       [ 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.weight.to_dense().numpy() @ x.numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8c93f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06015173,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.06583074,  0.12741561,  0.1874524 ,  0.18189035,  0.        ],\n",
       "       [ 0.        , -0.05709397,  0.        ,  0.        , -0.04196466],\n",
       "       [-0.11316157,  0.        ,  0.30750433,  0.        , -0.06821091],\n",
       "       [ 0.        , -0.01736058,  0.        , -0.20325258,  0.        ],\n",
       "       [ 0.2300624 ,  0.3708258 ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.weight.to_dense().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe852b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6d12ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
       "                       [0, 0, 1, 2, 3, 1, 4, 5, 2, 4, 6, 3, 5, 6]]),\n",
       "       values=tensor([-0.0602,  0.0658,  0.1274,  0.1875,  0.1819, -0.0571,\n",
       "                      -0.0420, -0.1132,  0.3075, -0.0682, -0.0174, -0.2033,\n",
       "                       0.2301,  0.3708]),\n",
       "       size=(7, 5), nnz=14, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99c293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of target: torch.Size([7, 5])\n",
      "Shape of inputs.t(): torch.Size([5, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "addmm: index out of column bound: 5 not between 1 and 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(len_input):\n\u001b[1;32m      7\u001b[0m     x[:, i] \u001b[38;5;241m=\u001b[39m x[:, i] \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m output\n",
      "File \u001b[0;32m~/miniconda3/envs/lobframe/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 121\u001b[0m, in \u001b[0;36mSparseLinear.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mflatten(end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    116\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msparse_coo_tensor(\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices,\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights,\n\u001b[1;32m    119\u001b[0m     torch\u001b[38;5;241m.\u001b[39mSize([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features]),\n\u001b[1;32m    120\u001b[0m )\n\u001b[0;32m--> 121\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: addmm: index out of column bound: 5 not between 1 and 5"
     ]
    }
   ],
   "source": [
    "num_batches = 3\n",
    "x = torch.ones(3, len_input)\n",
    "x[1, :] = 2\n",
    "x[2, :] = 3\n",
    "\n",
    "for i in range(len_input):\n",
    "    x[:, i] = x[:, i] + i / 10\n",
    "\n",
    "output = sl(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da128073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 1, 100, 6])\n",
      "Output shape: torch.Size([4, 8, 100, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvFilter(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            padding=0,  # No padding for exact 2x downsampling\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, height, width)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "batch_size = 4\n",
    "in_channels = 1\n",
    "out_channels = 8\n",
    "rows = 100\n",
    "cols = 6\n",
    "\n",
    "# Create model\n",
    "conv1_tetrahedra = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=8,\n",
    "        kernel_size=(1, 2),\n",
    "        stride=(1, 2),\n",
    "    ),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "# Create batch of images\n",
    "x = torch.randn(batch_size, in_channels, rows, cols)\n",
    "print(f\"Input shape: {x.shape}\")  # torch.Size([4, 3, 32, 32])\n",
    "\n",
    "# Apply convolution\n",
    "output = conv1_tetrahedra(x)\n",
    "print(f\"Output shape: {output.shape}\")  # torch.Size([4, 16, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f5c48bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7730, 0.0756, 0.0871, 0.0632, 0.0000, 0.1362, 0.0000, 0.2796, 0.1956,\n",
      "         0.0000, 0.2654, 0.0000]], grad_fn=<CatBackward0>)\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class HNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        num_edges: int,\n",
    "        num_triangles: int,\n",
    "        num_tetrahedra: int,\n",
    "        nodes_to_edges_connections: tuple,\n",
    "        edges_to_triangles_connections: tuple,\n",
    "        triangles_to_tetrahedra_connections: tuple,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        nodes_to_edges_connections: tuple of two lists, where the first list contains the indices of the edges\n",
    "        and the second list contains the indices of the nodes connected to those edges, such that the i-th node\n",
    "        in the first list is a member of the i-th edge in the second list.\n",
    "\n",
    "        Same for edges_to_triangles_connections and triangles_to_tetrahedra_connections\n",
    "        \"\"\"\n",
    "        super(HNN, self).__init__()\n",
    "        self.sparse_layer_edges = SparseLinear(\n",
    "            num_nodes,\n",
    "            num_edges,\n",
    "            connectivity=torch.tensor(\n",
    "                [nodes_to_edges_connections[1], nodes_to_edges_connections[0]],\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.sparse_layer_triangles = SparseLinear(\n",
    "            num_edges,\n",
    "            num_triangles,\n",
    "            connectivity=torch.tensor(\n",
    "                [edges_to_triangles_connections[1], edges_to_triangles_connections[0]],\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.triangles_to_tetrahedra_connections = triangles_to_tetrahedra_connections\n",
    "\n",
    "        if len(self.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            self.sparse_layer_tetrahedra = SparseLinear(\n",
    "                num_triangles,\n",
    "                num_tetrahedra,\n",
    "                connectivity=torch.tensor(\n",
    "                    [\n",
    "                        triangles_to_tetrahedra_connections[1],\n",
    "                        triangles_to_tetrahedra_connections[0],\n",
    "                    ],\n",
    "                    dtype=torch.int64,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.sparse_layer_tetrahedra = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s1 = F.relu(self.sparse_layer_edges(x))\n",
    "\n",
    "        x_s2 = F.relu(self.sparse_layer_triangles(x_s1))\n",
    "\n",
    "        if len(self.triangles_to_tetrahedra_connections[0]) != 0:\n",
    "            x_s3 = F.relu(self.sparse_layer_tetrahedra(x_s2))\n",
    "\n",
    "            return torch.cat([x_s1, x_s2, x_s3], 1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return torch.cat([x_s1, x_s2], 1)\n",
    "\n",
    "\n",
    "hnn = HNN(\n",
    "    num_nodes=len(clique_1),\n",
    "    num_edges=len(clique_2),\n",
    "    num_triangles=len(clique_3),\n",
    "    num_tetrahedra=len(clique_4),\n",
    "    nodes_to_edges_connections=connection_1,\n",
    "    edges_to_triangles_connections=connection_2,\n",
    "    triangles_to_tetrahedra_connections=connection_3,\n",
    ")\n",
    "\n",
    "x = torch.ones(1, len(clique_1))\n",
    "output = hnn(x)\n",
    "print(output)\n",
    "print(output.shape)  # Should print the shape of the output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ca78976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clique_2), len(clique_3), len(clique_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e484a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(connection_1[1]) + 1, max(connection_2[1]) + 1, max(connection_3[1]) + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lobframe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
